{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48923cb0-8be8-47ad-bb90-ffdaaa1a98db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234ccb09-7366-4692-ab0c-2818cd321993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 필요 라이브러리 import 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1aa753-a5f0-4cc3-b37c-241d87721971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sin, cos, pi, lag, avg,year,max as spark_max, min as spark_min, expr, when, concat\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "from databricks import automl\n",
    "import databricks.automl as db_automl\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b15fe82-5921-41bc-8d02-b775ba2c67a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.option(\"header\", \"true\").csv('/Volumes/project/default/project/UHI_data.csv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e6fd4f-723f-442e-ae49-90647c0c0830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df.withColumn(\"green_rate\", col(\"green_rate\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Building_Density\", col(\"Building_Density\").cast(\"double\")) \\\n",
    "                 .withColumn(\"car_registration_count\", col(\"car_registration_count\").cast(\"long\")) \\\n",
    "                 .withColumn(\"population_density\", col(\"population_density\").cast(\"double\")) \\\n",
    "                 .withColumn(\"avg_km_per_road_km\", col(\"avg_km_per_road_km\").cast(\"double\")) \\\n",
    "                 .withColumn(\"UHII\", col(\"UHII\").cast(\"double\")) \\\n",
    "                 .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                 .withColumn(\"hour_sin\", col(\"hour_sin\").cast(\"double\")) \\\n",
    "                 .withColumn(\"hour_cos\", col(\"hour_cos\").cast(\"double\")) \\\n",
    "                 .withColumn(\"day_of_week_sin\", col(\"day_of_week_sin\").cast(\"double\")) \\\n",
    "                 .withColumn(\"day_of_week_cos\", col(\"day_of_week_cos\").cast(\"double\")) \\\n",
    "                 .withColumn(\"is_night_time\", col(\"is_night_time\").cast(\"int\")) \\\n",
    "                 .withColumn(\"suburban_temp_current\", col(\"suburban_temp_current\").cast(\"double\")) \\\n",
    "                 .withColumn(\"suburban_temp_lag_24hr\", col(\"suburban_temp_lag_24hr\").cast(\"double\")) \\\n",
    "                 .withColumn(\"suburban_temp_ma_24hr\", col(\"suburban_temp_ma_24hr\").cast(\"double\")) \\\n",
    "                 .withColumn(\"suburban_temp_lag_3hr\", col(\"suburban_temp_lag_3hr\").cast(\"double\")) \\\n",
    "                 .withColumn(\"UHII_lag_8\", col(\"UHII_lag_8\").cast(\"double\")) \\\n",
    "                 .withColumn(\"UHII_lag_1\", col(\"UHII_lag_1\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4cc8b1-9843-4fd8-9b65-bcb1005332b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### feature column 선택 및 vectorassembler 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "084b6a48-c604-400f-af02-8aac9a161e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 레이블 컬럼 정의\n",
    "LABEL_COL = 'UHII'\n",
    "\n",
    "# 범주형 피처 정의\n",
    "categorical_cols = ['District'] \n",
    "\n",
    "# 수치형 피처 정의\n",
    "numerical_cols = [\n",
    "    'green_rate',\n",
    "    'Building_Density',\n",
    "    'car_registration_count',\n",
    "    'population_density',\n",
    "    'avg_km_per_road_km',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'is_night_time',\n",
    "    'suburban_temp_current',\n",
    "    'suburban_temp_lag_24hr',\n",
    "    'suburban_temp_ma_24hr',\n",
    "    'suburban_temp_lag_3hr',\n",
    "    'UHII_lag_8',\n",
    "    'UHII_lag_1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb97deb5-7864-4a74-80b3-eea6753dc794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 파이프라인 스테이지 정의\n",
    "stages = []\n",
    "\n",
    "# StringIndexer: 문자열 카테고리를 숫자 인덱스로 변환\n",
    "indexed_district_col = 'District_indexed'\n",
    "string_indexer = StringIndexer() \\\n",
    "    .setInputCol('District') \\\n",
    "    .setOutputCol(indexed_district_col) \\\n",
    "    .setHandleInvalid(\"skip\") # 'skip'은 학습 데이터에 없는 카테고리를 무시합니다.\n",
    "stages.append(string_indexer)\n",
    "\n",
    "# VectorAssembler: 모든 피처를 하나의 벡터로 묶기\n",
    "final_feature_cols = numerical_cols\n",
    "\n",
    "assembler = VectorAssembler() \\\n",
    "    .setInputCols(final_feature_cols) \\\n",
    "    .setOutputCol(\"features\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# MLlib Pipeline 생성 및 적용\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# 이 시점에는 df_processed가 이미 준비되어 있어야 합니다.\n",
    "# (df_processed = 이전에 CSV를 읽고 피처 엔지니어링이 완료된 DataFrame)\n",
    "pipeline_model = pipeline.fit(df_processed)\n",
    "assembled_df = pipeline_model.transform(df_processed)\n",
    "\n",
    "print(\"--- 파이프라인 적용 후 DataFrame 스키마 ---\")\n",
    "assembled_df.printSchema()\n",
    "print(\"\\n--- 파이프라인 적용 후 DataFrame (일부) ---\")\n",
    "# 'features' 컬럼에 모든 전처리된 피처들이 벡터 형태로 들어있을 겁니다.\n",
    "assembled_df.select('District', 'timestamp', 'features', LABEL_COL).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafd572d-e320-4b2b-8c75-d0a657d585f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML 실행\n",
    "\n",
    "# 여기서는 assembled_df를 사용하겠습니다.\n",
    "# 주의: Databricks AutoML은 내부적으로 데이터 분할을 수행하므로,\n",
    "# 이미 분할된 train_df, val_df, test_df를 합쳐서 사용하거나,\n",
    "# 원본 assembled_df를 그대로 사용하는 것이 좋습니다.\n",
    "# 여기서는 assembled_df를 사용한다고 가정합니다.\n",
    "# 만약 메모리 문제가 있다면 sampled_df를 사용하거나, train_df를 사용하고 validation_df를 지정할 수 있습니다.\n",
    "\n",
    "print(\"--- Databricks AutoML (회귀) 시작 ---\")\n",
    "\n",
    "# 'target_col': 예측하려는 대상 컬럼 (UHII)\n",
    "# 'timeout_minutes': AutoML이 모델을 탐색할 최대 시간 (분 단위)\n",
    "# 'country_cols': 지역 특성을 가진 범주형 컬럼 (AutoML이 알아서 처리하게 둠)\n",
    "# 'primary_metric': 최적화할 주된 평가 지표 (예: \"rmse\", \"r2\", \"mae\")\n",
    "# 'exclude_models': 제외할 모델 목록 (필요한 경우)\n",
    "# 'force_alpha_spark_ml': Spark MLlib 모델 사용 시 alpha 파라미터 적용 여부\n",
    "\n",
    "summary = db_automl.regress(\n",
    "    dataset=assembled_df,        # 전처리 및 피처 벡터화가 완료된 전체 데이터셋\n",
    "    target_col=LABEL_COL,        # 'UHII'\n",
    "    timeout_minutes=60,          # 1시간 동안 최적의 모델 탐색 (조절 가능)\n",
    "    primary_metric=\"rmse\",       # 주된 최적화 지표를 RMSE로 설정\n",
    "    # 여기서는 District가 이미 원핫 인코딩 되었으므로 categorical_cols를 명시할 필요는 없지만,\n",
    "    # 만약 원핫 인코딩 이전의 원본 District 컬럼이 있었다면 아래처럼 지정할 수 있습니다.\n",
    "    # categorical_cols=[\"District\"]\n",
    "    # exclude_models=['sklearn.linear_model.LinearRegression'] # 특정 모델 제외 예시\n",
    ")\n",
    "\n",
    "print(\"\\n--- Databricks AutoML 완료 ---\")\n",
    "\n",
    "# AutoML 결과 확인\n",
    "# summary 객체는 최적의 모델, 실험 결과 등에 대한 정보를 포함합니다.\n",
    "# 최적의 모델을 재현하는 노트북 링크를 얻을 수 있습니다.\n",
    "print(f\"최적의 모델 링크: {summary.best_model_notebook_url}\")\n",
    "print(f\"모든 실험 결과 링크: {summary.experiment.url}\")\n",
    "\n",
    "# best_model_notebook_url을 클릭하여 자동으로 생성된 노트북을 열고 코드를 확인해 보세요.\n",
    "# 이 노트북에는 최적의 모델을 로드하고 예측을 수행하는 코드가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbd19de-af84-4218-b093-15e2ae847732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- . 학습/검증/테스트 데이터셋 분할 (시간 순서 고려) ---\n",
    "print(\"\\n--- 4. 학습/검증/테스트 데이터셋 분할 (시간 순서 고려) ---\")\n",
    "min_date, max_date = assembled_df.agg(\n",
    "    spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))\n",
    ").head()\n",
    "print(f\"전체 데이터 기간: {min_date} ~ {max_date}\")\n",
    "\n",
    "# 시계열 데이터이므로 timestamp 기준으로 정렬 후 분할합니다.\n",
    "sorted_df = assembled_df.orderBy(\"timestamp\")\n",
    "train_df, val_df, test_df = sorted_df.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {train_df.count()} 행\")\n",
    "print(f\"검증 데이터셋 크기: {val_df.count()} 행\")\n",
    "print(f\"테스트 데이터셋 크기: {test_df.count()} 행\")\n",
    "\n",
    "if train_df.count() > 0:\n",
    "    train_min_date, train_max_date = train_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  학습 데이터 기간: {train_min_date} ~ {train_max_date}\")\n",
    "if val_df.count() > 0:\n",
    "    val_min_date, val_max_date = val_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  검증 데이터 기간: {val_min_date} ~ {val_max_date}\")\n",
    "if test_df.count() > 0:\n",
    "    test_min_date, test_max_date = test_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  테스트 데이터 기간: {test_min_date} ~ {test_max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c36c1a6-a5d9-4312-a11a-76ed67b0984a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Regressor 모델 정의\n",
    "xgb_regressor = SparkXGBRegressor(\n",
    "    features_col=\"features\", # 이미 assembled_df에 features 컬럼이 있으므로 이 이름을 사용\n",
    "    label_col=\"UHII\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 모델 학습 시에는 StringIndexer, OneHotEncoder, VectorAssembler는 이미 적용되었으므로\n",
    "# 모델 자체만 파이프라인에 넣거나, 바로 모델을 fit합니다.\n",
    "# 여기서는 Pipeline 대신 바로 모델을 fit하는 방식을 선택합니다.\n",
    "\n",
    "print(\"\\n--- XGBoost 모델 학습 시작 (assembled_df로부터) ---\")\n",
    "# assembled_df를 train_df, val_df, test_df로 분할했으므로,\n",
    "# 이제 train_df는 이미 features 컬럼을 가지고 있습니다.\n",
    "# 따라서, xgb_regressor 모델 인스턴스만 사용하여 학습을 수행합니다.\n",
    "model_xgb_fitted = xgb_regressor.fit(train_df) # train_df는 이미 features 컬럼을 가짐\n",
    "print(\"--- XGBoost 모델 학습 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b0cc94-50d8-40dd-971a-6e1021de6026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML이 찾아준 최적의 LightGBM 하이퍼파라미터\n",
    "space = {\n",
    "    \"colsample_bytree\": 0.7093243554323677,\n",
    "    \"lambda_l1\": 0.5657063073349601,\n",
    "    \"lambda_l2\": 0.26202557960159445,\n",
    "    \"learning_rate\": 0.03633882162016305,\n",
    "    \"max_bin\": 486,\n",
    "    \"max_depth\": 11,\n",
    "    \"min_child_samples\": 59,\n",
    "    \"n_estimators\": 739,\n",
    "    \"num_leaves\": 161,\n",
    "    \"subsample\": 0.5900433909418646,\n",
    "    \"random_state\": 930035991,\n",
    "}\n",
    "\n",
    "# LightGBMRegressor 모델 정의\n",
    "# 파라미터 이름은 \"Best Trial Notebook\"에서 정확히 확인하는 것이 좋습니다.\n",
    "# 여기서는 synapse.ml.lightgbm의 일반적인 파라미터 이름을 사용합니다.\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"UHII\",\n",
    "    numIterations=space[\"n_estimators\"], # n_estimators -> numIterations\n",
    "    learningRate=space[\"learning_rate\"],\n",
    "    maxDepth=space[\"max_depth\"],\n",
    "    numLeaves=space[\"num_leaves\"],\n",
    "    minChildSamples=space[\"min_child_samples\"],\n",
    "    subsample=space[\"subsample\"],\n",
    "    colsampleByTree=space[\"colsample_bytree\"], # colsampleByTree\n",
    "    regAlpha=space[\"lambda_l1\"], # lambda_l1 -> regAlpha\n",
    "    regLambda=space[\"lambda_l2\"], # lambda_l2 -> regLambda\n",
    "    seed=space[\"random_state\"],\n",
    "    # objective=\"regression\" # 명시적으로 회귀 문제임을 지정할 수 있습니다.\n",
    ")\n",
    "\n",
    "# LightGBM Estimator를 단일 스테이지 Pipeline에 포함\n",
    "# train_df는 이미 features와 UHII 컬럼을 가지고 있으므로, 추가적인 전처리 스테이지는 필요 없습니다.\n",
    "pipeline_lgbm = Pipeline(stages=[lgbm_regressor])\n",
    "\n",
    "print(\"\\n--- LightGBM 모델 학습 시작 (Pipeline 사용) ---\")\n",
    "# 파이프라인을 train_df에 fit합니다.\n",
    "# 파이프라인이 내부적으로 lgbm_regressor.fit(train_df)를 올바른 MLlib 방식으로 호출합니다.\n",
    "model_lgbm_fitted = pipeline_lgbm.fit(train_df)\n",
    "print(\"--- LightGBM 모델 학습 완료 ---\")\n",
    "\n",
    "# --- 모델 평가 (이전 코드와 동일하게 유지) ---\n",
    "# 검증 데이터셋으로 예측 수행\n",
    "predictions_val_lgbm = model_lgbm_fitted.transform(val_df)\n",
    "\n",
    "# 테스트 데이터셋으로 예측 수행\n",
    "predictions_test_lgbm = model_lgbm_fitted.transform(test_df)\n",
    "\n",
    "# RegressionEvaluator를 사용하여 모델 평가\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"UHII\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"\\n검증 데이터셋 RMSE (LightGBM): {rmse_val_lgbm:.4f}\")\n",
    "\n",
    "rmse_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 RMSE (LightGBM): {rmse_test_lgbm:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"r2\")\n",
    "r2_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"검증 데이터셋 R2 (LightGBM): {r2_val_lgbm:.4f}\")\n",
    "\n",
    "r2_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 R2 (LightGBM): {r2_test_lgbm:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"검증 데이터셋 MAE (LightGBM): {mae_val_lgbm:.4f}\")\n",
    "\n",
    "mae_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 MAE (LightGBM): {mae_test_lgbm:.4f}\")\n",
    "\n",
    "print(\"\\n--- LightGBM 모델 평가 완료 ---\")\n",
    "predictions_test_lgbm.select(\"UHII\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec51f570-868f-4774-8e50-ed66deeec028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 검증 데이터셋으로 예측 수행\n",
    "# val_df 또한 이미 features 컬럼을 가지고 있습니다.\n",
    "predictions_val_xgb = model_lgbm_fitted.transform(val_df)\n",
    "\n",
    "# 테스트 데이터셋으로 예측 수행\n",
    "# test_df 또한 이미 features 컬럼을 가지고 있습니다.\n",
    "predictions_test_xgb = model_lgbm_fitted.transform(test_df)\n",
    "\n",
    "# RegressionEvaluator를 사용하여 모델 평가 (이 부분은 동일)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"UHII\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"\\n검증 데이터셋 RMSE (XGBoost): {rmse_val_xgb:.4f}\")\n",
    "\n",
    "rmse_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 RMSE (XGBoost): {rmse_test_xgb:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"r2\")\n",
    "r2_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"검증 데이터셋 R2 (XGBoost): {r2_val_xgb:.4f}\")\n",
    "\n",
    "r2_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 R2 (XGBoost): {r2_test_xgb:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"검증 데이터셋 MAE (XGBoost): {mae_val_xgb:.4f}\")\n",
    "\n",
    "mae_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 MAE (XGBoost): {mae_test_xgb:.4f}\")\n",
    "\n",
    "print(\"\\n--- XGBoost 모델 평가 완료 ---\")\n",
    "predictions_test_xgb.select(\"UHII\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddd5a94-db18-4721-b5e4-3ad77877cbf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- UHII와 수치형 피처 간의 상관관계 ---\")\n",
    "\n",
    "# UHII와 각 수치형 피처 간의 피어슨 상관계수 계산\n",
    "for col_name in numerical_cols:\n",
    "    try:\n",
    "        correlation = assembled_df.stat.corr(LABEL_COL, col_name)\n",
    "        print(f\"UHII 와 {col_name:<25}: {correlation:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating correlation for {col_name}: {e}\")\n",
    "\n",
    "# --- 수치형 피처 및 UHII 간의 상관관계 행렬 (Pandas 변환, 메모리 주의!) ---\n",
    "# 데이터셋의 크기가 크지 않다면 이 방법이 편리합니다.\n",
    "# 대용량 데이터셋에서는 Spark의 RDD 또는 VectorAssembler 후 행렬 연산 등의 고급 기법이 필요할 수 있습니다.\n",
    "# 여기서는 예시로 train_df를 사용합니다. 실제로는 전체 assembled_df를 사용하거나 적절히 샘플링하여 사용하세요.\n",
    "\n",
    "cols_for_corr_matrix = numerical_cols + [LABEL_COL]\n",
    "\n",
    "try:\n",
    "    # 필요한 컬럼만 선택하여 Pandas DataFrame으로 변환\n",
    "    pandas_df_for_corr = assembled_df.select(cols_for_corr_matrix).toPandas()\n",
    "\n",
    "    print(\"\\n--- 수치형 피처 및 UHII 간의 상관관계 행렬 (Pandas) ---\")\n",
    "    correlation_matrix = pandas_df_for_corr.corr(method='pearson')\n",
    "    #print(correlation_matrix.round(2)) # 소수점 두 자리까지 표시\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError converting to Pandas for correlation matrix or calculation: {e}\")\n",
    "    print(\"Consider sampling the DataFrame or using Spark-native correlation methods if the dataset is too large.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone)2 ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
