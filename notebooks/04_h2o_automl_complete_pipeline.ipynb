{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35f466a-c0a5-451b-84f6-4bb962769e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import random # H2O 초기화 시 포트 번호 충돌 방지를 위해 사용\n",
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "\n",
    "# 1. Spark Session 초기화 (Databricks에서는 보통 자동으로 되어 있습니다)\n",
    "# spark = SparkSession.builder.appName(\"H2OAutoMLExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c96f565-1fbe-4280-a0fe-0c0458f3f2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. CSV 데이터 불러오기\n",
    "# 데이터 경로를 실제 환경에 맞게 수정해주세요.\n",
    "csv_path = '/Volumes/project/default/project/UHI_data_test.csv/'\n",
    "df = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "\n",
    "print(\"--- 원본 Spark DataFrame 스키마 ---\")\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7493f9f8-e4d2-4eb7-9021-ea52ac8cc3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. 데이터 타입 변환 (df_processed 생성)\n",
    "# H2O AutoML이 datetime 타입의 timestamp를 인식하도록 cast 합니다.\n",
    "df_processed = df.withColumn(\"green_rate\", col(\"green_rate\").cast(\"double\")) \\\n",
    "                  .withColumn(\"Building_Density\", col(\"Building_Density\").cast(\"double\")) \\\n",
    "                  .withColumn(\"car_registration_count\", col(\"car_registration_count\").cast(\"long\")) \\\n",
    "                  .withColumn(\"population_density\", col(\"population_density\").cast(\"double\")) \\\n",
    "                  .withColumn(\"avg_km_per_road_km\", col(\"avg_km_per_road_km\").cast(\"double\")) \\\n",
    "                  .withColumn(\"UHII\", col(\"UHII\").cast(\"double\")) \\\n",
    "                  .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                  .withColumn(\"suburban_temp_current\", col(\"suburban_temp_current\").cast(\"double\"))\n",
    "\n",
    "print(\"\\n--- 타입 변환 후 Spark DataFrame 스키마 ---\")\n",
    "df_processed.printSchema()\n",
    "df_processed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464d493f-920a-44fe-ac2b-efc17c6e6a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Spark DataFrame을 Pandas DataFrame으로 변환\n",
    "# H2O는 Pandas DataFrame 또는 H2OFrame을 입력으로 받습니다.\n",
    "# 대규모 데이터셋의 경우 메모리 부족에 주의해야 합니다.\n",
    "try:\n",
    "    pandas_df = df_processed.toPandas()\n",
    "    print(f\"\\nSpark DataFrame을 Pandas DataFrame으로 변환 완료. 크기: {pandas_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Pandas DataFrame 변환 중 오류 발생: {e}\")\n",
    "    print(\"데이터셋이 너무 커서 메모리 부족이 발생했을 수 있습니다. 샘플링을 고려하세요.\")\n",
    "    # 예: pandas_df = df_processed.sample(fraction=0.1, seed=42).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b544535f-9d01-4b88-b5e5-80774306fb42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. H2O 초기화\n",
    "# Databricks에서 H2O를 초기화할 때 포트 충돌이 발생할 수 있으므로, 랜덤 포트를 사용하거나 특정 포트를 지정하는 것이 좋습니다.\n",
    "# cluster_mode=True 설정은 H2O를 Spark 클러스터에서 실행합니다. (권장)\n",
    "# nthreads=-1은 사용 가능한 모든 코어를 사용하겠다는 의미입니다.\n",
    "try:\n",
    "    h2o.init(\n",
    "        # ip=\"localhost\", # 로컬 IP 주소 지정 (선택 사항)\n",
    "        port=random.randint(54321, 65535), # 랜덤 포트 사용\n",
    "        nthreads=-1, # 사용 가능한 모든 스레드 사용\n",
    "        min_mem_size=\"4G\", # H2O에 할당할 최소 메모리 크기 (클러스터 환경에 맞게 조정)\n",
    "        # spark_session=spark, # SparkSession 지정 (Spark 클러스터에서 H2O 실행 시)\n",
    "        # strict_version_check=False # H2O 버전 체크 비활성화 (환경 문제시)\n",
    "    )\n",
    "    print(\"\\nH2O Flow UI: %s\" % h2o.cluster().get_web_ui())\n",
    "except Exception as e:\n",
    "    print(f\"H2O 초기화 중 오류 발생: {e}\")\n",
    "    print(\"H2O 클러스터가 이미 실행 중이거나 포트 충돌이 발생했을 수 있습니다.\")\n",
    "    print(\"h2o.shutdown() 후 다시 시도하거나, 다른 포트 번호를 사용해보세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc81ef47-f2e7-4f93-a0b2-19e9b3676775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Pandas DataFrame을 H2OFrame으로 변환\n",
    "h2o_frame = h2o.H2OFrame(pandas_df)\n",
    "\n",
    "print(\"\\nH2OFrame 정보:\")\n",
    "h2o_frame.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "610b7766-897f-4f4a-b3f5-96178f38e437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. 예측 대상 (y) 및 피처 (x) 정의\n",
    "y = 'UHII' # 예측하려는 컬럼\n",
    "x = [col for col in h2o_frame.columns if col != y]\n",
    "\n",
    "# H2OFrame의 컬럼 타입을 확인하는 올바른 방법은 .types 딕셔너리를 사용하는 것입니다.\n",
    "timestamp_col_type = h2o_frame.types.get('timestamp')\n",
    "\n",
    "print(f\"\\n'{y}'는 예측 대상입니다.\")\n",
    "print(f\"사용될 피처들 ({len(x)}개): {x}\")\n",
    "print(f\"H2OFrame의 'timestamp' 컬럼 타입: {timestamp_col_type}\") # 올바른 타입 출력\n",
    "\n",
    "# 이제 이 'timestamp_col_type' 변수를 사용하여 조건을 확인합니다.\n",
    "if timestamp_col_type != 'time':\n",
    "    print(\"경고: 'timestamp' 컬럼이 'time' 타입으로 인식되지 않았습니다. 변환을 다시 확인하거나 데이터 형식을 점검하세요.\")\n",
    "    # 만약 여전히 'time'이 아니라면, 다음 줄의 주석을 해제하여 강제로 'time' 타입으로 변환 시도\n",
    "    # h2o_frame['timestamp'] = h2o_frame['timestamp'].as_time()\n",
    "    # print(f\"강제 변환 후 'timestamp' 컬럼 타입: {h2o_frame.types.get('timestamp')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8378fe14-2e3e-4707-bc93-bf9dee37e7d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. AutoML 모델 학습 설정 및 실행\n",
    "# max_runtime_secs: 최대 실행 시간 (초). 시간 부족 시 모델 수가 적을 수 있습니다.\n",
    "# max_models: 최대 훈련할 모델 수.\n",
    "# seed: 재현성을 위한 시드 값.\n",
    "# sort_metric: 리더보드를 정렬하고 최적 모델을 선택할 때 사용할 평가 지표. (회귀: rmse, mae, rmsle, mean_residual_deviance)\n",
    "# exclude_algos: 특정 알고리즘 제외 (선택 사항).\n",
    "# include_algos: 특정 알고리즘만 포함 (선택 사항).\n",
    "# nfolds: 교차 검증 폴드 수.\n",
    "# verbosity: 상세 출력 수준 (debug, info, warn, error).\n",
    "\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=7200,  # 예: 1시간 (3600초) 동안 실행\n",
    "    max_models=40,         # 예: 최대 20개의 모델 훈련\n",
    "    seed=42,                # 재현성을 위한 시드\n",
    "    sort_metric=\"rmse\",     # 회귀 문제이므로 RMSE 사용\n",
    "    # exclude_algos=[\"DeepLearning\", \"GLM\"], # 특정 알고리즘 제외 예시\n",
    "    # nfolds=5, # 교차 검증을 5-fold로 설정 (선택 사항, 기본값은 자동으로 설정될 수 있음)\n",
    "    # verbosity=\"info\", # 상세 정보 출력\n",
    "    # project_name=\"UHI_Prediction\" # 프로젝트 이름 지정\n",
    ")\n",
    "\n",
    "print(\"\\n--- H2O AutoML 학습 시작 ---\")\n",
    "aml.train(x=x, y=y, training_frame=h2o_frame)\n",
    "\n",
    "print(\"\\n--- H2O AutoML 학습 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908c6012-5151-4263-98ad-ddb29587ca1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9. 리더보드 확인 (최고 성능 모델 목록)\n",
    "leaderboard = aml.leaderboard\n",
    "print(\"\\n--- AutoML Leaderboard ---\")\n",
    "leaderboard.head(rows=10) # 상위 10개 모델 출력\n",
    "\n",
    "# 10. 최고 성능 모델 확인\n",
    "best_model = aml.leader\n",
    "print(f\"\\n최고 성능 모델: {best_model.model_id}\")\n",
    "\n",
    "# 모델 성능 평가 (학습 데이터 기준)\n",
    "print(f\"\\n--- 최고 모델 ({best_model.model_id})의 학습 데이터 기준 성능 ---\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {best_model.rmse(train=True)}\")\n",
    "print(f\"R2 (R-squared): {best_model.r2(train=True)}\")\n",
    "print(f\"MAE (Mean Absolute Error): {best_model.mae(train=True)}\")\n",
    "print(f\"RMSLE (Root Mean Squared Logarithmic Error): {best_model.rmsle(train=True)}\") # 필요한 경우 사용\n",
    "print(f\"MRD (Mean Residual Deviance): {best_model.mean_residual_deviance(train=True)}\")\n",
    "\n",
    "\n",
    "# 만약 검증 데이터를 분리했다면:\n",
    "# print(f\"최고 모델 ({best_model.model_id})의 검증 데이터 RMSE: {best_model.rmse(valid=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "88c6f284-6fa3-46ba-bb2e-4548e119e7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 피쳐 중요도\n",
    "# (이전 코드에서 H2OAutoML 학습 완료 후 best_model 객체가 있다고 가정)\n",
    "# best_model = aml.leader\n",
    "\n",
    "# 피처 중요도 테이블 가져오기\n",
    "varimp_df = best_model.varimp(use_pandas=True)\n",
    "\n",
    "print(\"\\n--- 최고 모델의 피처 중요도 (상위 10개) ---\")\n",
    "# Importance, Percentage, Cumulative Sum 컬럼이 포함됩니다.\n",
    "print(varimp_df.head(10))\n",
    "\n",
    "# 전체 피처 중요도를 보고 싶다면\n",
    "# print(varimp_df)\n",
    "\n",
    "# 중요도 순으로 정렬되어 있으므로, 처음 몇 개만 봐도 주요 피처를 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfedcf9-8068-416b-ad71-a7d273c53a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11. 예측 수행 (새로운 데이터 또는 기존 데이터로 테스트)\n",
    "# 테스트 데이터셋이 있다고 가정하고, 동일한 전처리 (type casting)만 수행한 후 사용합니다.\n",
    "# 예를 들어, test_df_processed = spark.read.option(\"header\", \"true\").csv('/path/to/test_data.csv')...\n",
    "# test_pandas_df = test_df_processed.toPandas()\n",
    "# test_h2o_frame = h2o.H2OFrame(test_pandas_df)\n",
    "\n",
    "# 예시: 학습 데이터를 다시 사용하여 예측 (실제로는 이렇게 하지 않습니다)\n",
    "predictions = best_model.predict(h2o_frame)\n",
    "print(\"\\n--- 예측 결과 (일부) ---\")\n",
    "predictions.head(5)\n",
    "\n",
    "# 예측 결과를 Pandas DataFrame으로 변환\n",
    "predictions_pandas = predictions.as_data_frame()\n",
    "print(\"\\n--- 예측 결과 Pandas DataFrame (일부) ---\")\n",
    "print(predictions_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c3d213-11f9-483f-8faf-33ac1442b7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_path = h2o.save_model(model=best_model, path=\"/tmp/mymodel\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d249ac94-56c3-4f2d-b150-f5989b27c532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model = h2o.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9c9864e6-da7a-4aa2-b2c2-a1815264fc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_data_spark_df_raw = spark.createDataFrame([\n",
    "    # 예측 데이터1\n",
    "    ('Jungnang-gu', 0.364, 1.178, 113569, 21597.946, 8848.239521, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터2\n",
    "    ('Dobong-gu', 0.569, 0.764, 96298, 15852.833, 5277.944143, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터3\n",
    "    ('Jung-gu', 0.255, 2.07, 53324, 13174.096, 14927.25044, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터4\n",
    "    ('Jung-gu', 0.1, 2.07, 53324, 13174.096, 14927.25044, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터5\n",
    "    ('Jung-gu', 0.7, 0.6, 53324, 13174.096, 14927.25044, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터6\n",
    "    ('Gangnam-gu', 0.7, 0.6, 53324, 13174.096, 14927.25044, '2020-08-01T18:00:00.000Z', 20.79),\n",
    "    # 예측 데이터7\n",
    "    ('Jung-gu', 0.255, 2.07, 53324, 13174.096, 14927.25044, '2024-07-12T15:00:00.000Z', 25.21),\n",
    "    # 예측 데이터8\n",
    "    ('Jung-gu', 0.255, 2.07, 53324, 13174.096, 14927.25044, '2024-07-17T03:00:00.000Z', 21.17),\n",
    "    # 예측 데이터9\n",
    "    ('Gangbuk-gu', 0.628, 0.555, 76105, 13202.076, 5240.425625, '2025-08-19T16:00:00.000Z', 28.9),\n",
    "    # 예측 데이터10\n",
    "    ('Gangnam-gu', 0.628, 0.555, 76105, 13202.076, 5240.425625, '2025-08-19T16:00:00.000Z', 28.9)\n",
    "], schema=[\n",
    "    'District', 'green_rate', 'Building_Density', 'car_registration_count',\n",
    "    'population_density', 'avg_km_per_road_km', 'timestamp', 'suburban_temp_current'\n",
    "])\n",
    "\n",
    "# 2. 학습 데이터와 동일한 Spark DataFrame 전처리 적용\n",
    "# 여기서 timestamp 컬럼을 Spark의 timestamp 타입으로 캐스팅합니다.\n",
    "new_data_spark_df_processed = new_data_spark_df_raw.withColumn(\"green_rate\", col(\"green_rate\").cast(\"double\")) \\\n",
    "                                                     .withColumn(\"Building_Density\", col(\"Building_Density\").cast(\"double\")) \\\n",
    "                                                     .withColumn(\"car_registration_count\", col(\"car_registration_count\").cast(\"long\")) \\\n",
    "                                                     .withColumn(\"population_density\", col(\"population_density\").cast(\"double\")) \\\n",
    "                                                     .withColumn(\"avg_km_per_road_km\", col(\"avg_km_per_road_km\").cast(\"double\")) \\\n",
    "                                                     .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                                                     .withColumn(\"suburban_temp_current\", col(\"suburban_temp_current\").cast(\"double\"))\n",
    "\n",
    "print(\"\\n--- 예측할 새로운 Spark DataFrame (전처리 후) ---\")\n",
    "new_data_spark_df_processed.show(truncate=False)\n",
    "new_data_spark_df_processed.printSchema()\n",
    "\n",
    "\n",
    "# 3. Spark DataFrame을 Pandas DataFrame으로 변환\n",
    "new_data_pandas_df = new_data_spark_df_processed.toPandas()\n",
    "\n",
    "# Pandas에서 datetime 타입으로 이미 잘 변환되었는지 확인\n",
    "print(\"\\n--- 예측할 새로운 Pandas DataFrame ---\")\n",
    "display(new_data_pandas_df)\n",
    "print(\"\\nTimestamp 컬럼 타입 (Pandas):\", new_data_pandas_df['timestamp'].dtype)\n",
    "\n",
    "\n",
    "# 4. Pandas DataFrame을 H2OFrame으로 변환 (예측 데이터)\n",
    "# 학습 시와 동일한 방식으로 H2O가 'timestamp' 컬럼을 'time' 타입으로 자동 추론하기를 기대합니다.\n",
    "new_h2o_frame = h2o.H2OFrame(new_data_pandas_df)\n",
    "\n",
    "\n",
    "# 5. H2OFrame의 'timestamp' 컬럼 타입 확인\n",
    "# H2O가 'time' 타입으로 잘 인식했는지 최종 확인합니다.\n",
    "h2o_frame_timestamp_type = new_h2o_frame.types.get('timestamp')\n",
    "print(f\"\\n변환 후 예측 데이터 'timestamp' 컬럼 H2O 타입: {h2o_frame_timestamp_type}\")\n",
    "\n",
    "if h2o_frame_timestamp_type != 'time':\n",
    "    print(\"경고: 'timestamp' 컬럼이 'time' 타입으로 인식되지 않았습니다. 모델 예측에 문제가 발생할 수 있습니다.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 예측할 새로운 H2OFrame 정보 ---\")\n",
    "new_h2o_frame.show_summary()\n",
    "\n",
    "\n",
    "# 6. 최고 성능 모델로 예측 수행\n",
    "predictions = best_model.predict(new_h2o_frame)\n",
    "\n",
    "# 7. 예측 결과를 Pandas DataFrame으로 변환하여 원본 데이터와 함께 보기\n",
    "predictions_df = predictions.as_data_frame()\n",
    "result_df = pd.concat([new_data_pandas_df, predictions_df], axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n--- 입력 데이터와 최종 UHII 예측 결과 ---\")\n",
    "display(result_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "H2O AutoML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
