{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48923cb0-8be8-47ad-bb90-ffdaaa1a98db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234ccb09-7366-4692-ab0c-2818cd321993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 필요 라이브러리 import 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1aa753-a5f0-4cc3-b37c-241d87721971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sin, cos, pi, lag, avg,year,max as spark_max, min as spark_min, expr, when, concat\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "from databricks import automl\n",
    "import databricks.automl as db_automl\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b15fe82-5921-41bc-8d02-b775ba2c67a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.option(\"header\", \"true\").csv(\"/Volumes/project/default/project/UHI_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9b63b6-fe06-4746-95d9-582876b436df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e6fd4f-723f-442e-ae49-90647c0c0830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df.withColumn(\"green_rate\", col(\"green_rate\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Building_Density\", col(\"Building_Density\").cast(\"double\")) \\\n",
    "                 .withColumn(\"car_registration_count\", col(\"car_registration_count\").cast(\"long\")) \\\n",
    "                 .withColumn(\"population_density\", col(\"population_density\").cast(\"double\")) \\\n",
    "                 .withColumn(\"avg_km_per_road_km\", col(\"avg_km_per_road_km\").cast(\"double\")) \\\n",
    "                 .withColumn(\"UHII\", col(\"UHII\").cast(\"double\")) \\\n",
    "                 .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                 .withColumn(\"suburban_temp_current\", col(\"suburban_temp_current\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4cc8b1-9843-4fd8-9b65-bcb1005332b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### feature column 선택 및 vectorassembler 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "084b6a48-c604-400f-af02-8aac9a161e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 레이블 컬럼 정의\n",
    "LABEL_COL = 'UHII'\n",
    "\n",
    "# 범주형 피처 정의\n",
    "categorical_cols = ['District'] \n",
    "\n",
    "# 수치형 피처 정의\n",
    "numerical_cols = [\n",
    "    'green_rate',\n",
    "    'Building_Density',\n",
    "    'car_registration_count',\n",
    "    'population_density',\n",
    "    'avg_km_per_road_km',\n",
    "    'suburban_temp_current'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb97deb5-7864-4a74-80b3-eea6753dc794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 파이프라인 스테이지 정의\n",
    "stages = []\n",
    "\n",
    "# StringIndexer: 문자열 카테고리를 숫자 인덱스로 변환\n",
    "indexed_district_col = 'District_indexed'\n",
    "string_indexer = StringIndexer() \\\n",
    "    .setInputCol('District') \\\n",
    "    .setOutputCol(indexed_district_col) \\\n",
    "    .setHandleInvalid(\"skip\") # 'skip'은 학습 데이터에 없는 카테고리를 무시합니다.\n",
    "stages.append(string_indexer)\n",
    "\n",
    "# VectorAssembler: 모든 피처를 하나의 벡터로 묶기\n",
    "final_feature_cols = numerical_cols\n",
    "\n",
    "assembler = VectorAssembler() \\\n",
    "    .setInputCols(final_feature_cols) \\\n",
    "    .setOutputCol(\"features\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# MLlib Pipeline 생성 및 적용\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# 이 시점에는 df_processed가 이미 준비되어 있어야 합니다.\n",
    "# (df_processed = 이전에 CSV를 읽고 피처 엔지니어링이 완료된 DataFrame)\n",
    "pipeline_model = pipeline.fit(df_processed)\n",
    "assembled_df = pipeline_model.transform(df_processed)\n",
    "\n",
    "print(\"--- 파이프라인 적용 후 DataFrame 스키마 ---\")\n",
    "assembled_df.printSchema()\n",
    "print(\"\\n--- 파이프라인 적용 후 DataFrame (일부) ---\")\n",
    "# 'features' 컬럼에 모든 전처리된 피처들이 벡터 형태로 들어있을 겁니다.\n",
    "assembled_df.select('District', 'timestamp', 'features', LABEL_COL).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4a26b1a9-84c1-4412-b7ea-e6ad0e5b4e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. df_processed의 전체 스키마 확인 (컬럼 이름과 데이터 타입 집중)\n",
    "print(\"--- df_processed 스키마 ---\")\n",
    "df_processed.printSchema()\n",
    "\n",
    "# 2. df_processed의 상위 5개 행 데이터 확인 (값들이 제대로 들어있는지)\n",
    "print(\"\\n--- df_processed 상위 5개 행 ---\")\n",
    "df_processed.show(5, truncate=False)\n",
    "\n",
    "# 3. LABEL_COL과 timestamp 컬럼을 제외한 모든 피처 컬럼 이름들을 확인\n",
    "# 이 리스트가 비어있다면, AutoML이 학습할 다른 피처가 없다는 의미입니다.\n",
    "all_cols_except_time_and_target = [\n",
    "    col for col in df_processed.columns\n",
    "    if col not in [LABEL_COL, \"timestamp\"]\n",
    "]\n",
    "print(f\"\\n--- timestamp와 {LABEL_COL}을 제외한 피처 컬럼들: ---\")\n",
    "print(all_cols_except_time_and_target)\n",
    "\n",
    "# 만약 여기에 예상했던 피처 컬럼들이 없다면, df_processed를 만드는 과정에서 누락된 것입니다.\n",
    "# 만약 있다면, 해당 컬럼들의 데이터 타입을 개별적으로 점검해야 합니다.\n",
    "\n",
    "# 4. 각 피처 컬럼의 고유값, 결측치, 통계량 확인 (의심되는 컬럼에 대해)\n",
    "# 예시: '활성녹지율' 컬럼에 대해\n",
    "# df_processed.select('활성녹지율').summary().show()\n",
    "# df_processed.filter(col('활성녹지율').isNull()).count()\n",
    "# df_processed.select('활성녹지율').distinct().count() # 고유값 개수\n",
    "\n",
    "# 5. 혹시 명시적으로 feature_columns를 넣는 파라미터가 다시 생겼는지 확인 (AutoML 버전 업데이트 가능성)\n",
    "# help(automl.forecast)를 다시 한번 실행하여 최신 파라미터 목록을 확인하세요.\n",
    "# 만약 feature_columns 파라미터가 있다면, 명시적으로 넣어주세요.\n",
    "# (하지만 제공해주신 help 출력에는 없었습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafd572d-e320-4b2b-8c75-d0a657d585f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML 실행\n",
    "print(\"--- Databricks AutoML (회귀) 시작 ---\")\n",
    "summary = db_automl.forecast(\n",
    "    dataset=df_processed,       # 전처리 및 피처 벡터화가 완료된 전체 데이터셋\n",
    "    target_col=LABEL_COL,       # 'UHII'\n",
    "    time_col=\"timestamp\",\n",
    "    identity_col=\"District\",\n",
    "    timeout_minutes=30,         # 1시간 동안 최적의 모델 탐색 (조절 가능)\n",
    "    primary_metric=\"rmse\",      # 주된 최적화 지표를 RMSE로 설정\n",
    ")\n",
    "\n",
    "print(\"\\n--- Databricks AutoML 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9582d86b-8c3b-4129-90f5-f3cacf8c71a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "help(db_automl.forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "646cdaf1-b934-47f3-adb4-0f560b6241a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# AutoML 결과 확인\n",
    "# summary 객체는 최적의 모델, 실험 결과 등에 대한 정보를 포함합니다.\n",
    "# 최적의 모델을 재현하는 노트북 링크를 얻을 수 있습니다.\n",
    "#print(f\"최적의 모델 링크: {summary.best_model_notebook_url}\")\n",
    "print(f\"모든 실험 결과 링크: {summary.experiment.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1ef7bc7-486f-4912-a924-0d63782cb678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 여기에 최적의 모델 객체를 받아오는 코드 추가 ---\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# 1. MlflowClient 인스턴스 생성\n",
    "client = MlflowClient()\n",
    "\n",
    "# 2. AutoML 실행이 포함된 MLflow Experiment ID 가져오기\n",
    "experiment_id = summary.experiment.experiment_id\n",
    "\n",
    "# 3. Experiment 내의 모든 런(runs)을 검색하여 최적의 런 찾기\n",
    "# 'metrics.rmse' (주된 최적화 지표)를 기준으로 오름차순(ASC) 정렬하여 가장 좋은 런을 가져옵니다.\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    # 수정 후 (RMSE는 낮을수록 좋으므로 ASC)\n",
    "    order_by=[\"metrics.var_root_mean_squared_error ASC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "best_model = None # 최적의 모델 객체를 저장할 변수 초기화\n",
    "\n",
    "if runs:\n",
    "    best_run = runs[0]\n",
    "    print(f\"\\n--- 최적의 MLflow 런 정보 ---\")\n",
    "    print(f\"최적의 MLflow 런 ID: {best_run.info.run_id}\")\n",
    "    print(f\"최적의 런 지표 (RMSE): {best_run.data.metrics.get('rmse', 'N/A')}\")\n",
    "\n",
    "    # 4. 최적의 런에 로깅된 모델 로드\n",
    "    # Databricks AutoML은 일반적으로 모델을 \"model\"이라는 이름으로 로깅합니다.\n",
    "    logged_model_path = f\"runs:/{best_run.info.run_id}/model\"\n",
    "    \n",
    "    try:\n",
    "        # MLflow API를 사용하여 모델 로드 (PySpark MLlib 모델의 경우)\n",
    "        # AutoML이 로깅하는 모델은 주로 PySpark MLlib PipelineModel 형태입니다.\n",
    "        best_model = mlflow.spark.load_model(logged_model_path)\n",
    "        print(\"\\n--- 최적의 PySpark MLlib 모델 객체가 성공적으로 로드되었습니다. ---\")\n",
    "        print(f\"로드된 모델 타입: {type(best_model)}\")\n",
    "        \n",
    "        # 이제 best_model을 사용하여 새로운 데이터에 대해 예측을 수행할 수 있습니다.\n",
    "        # 예: predictions = best_model.transform(new_data_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- 모델 로드 중 오류 발생 ---\")\n",
    "        print(f\"오류 메시지: {e}\")\n",
    "        print(f\"MLflow 런에 'model'이라는 이름으로 Spark MLlib 모델이 로깅되었는지 확인하세요.\")\n",
    "        print(f\"로그된 아티팩트 경로: {best_run.info.artifact_uri}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nExperiment ID {experiment_id}에서 최적의 런을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbd19de-af84-4218-b093-15e2ae847732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- . 학습/검증/테스트 데이터셋 분할 (시간 순서 고려) ---\n",
    "print(\"\\n--- 4. 학습/검증/테스트 데이터셋 분할 (시간 순서 고려) ---\")\n",
    "min_date, max_date = assembled_df.agg(\n",
    "    spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))\n",
    ").head()\n",
    "print(f\"전체 데이터 기간: {min_date} ~ {max_date}\")\n",
    "\n",
    "# 시계열 데이터이므로 timestamp 기준으로 정렬 후 분할합니다.\n",
    "sorted_df = assembled_df.orderBy(\"timestamp\")\n",
    "train_df, val_df, test_df = sorted_df.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
    "\n",
    "print(f\"학습 데이터셋 크기: {train_df.count()} 행\")\n",
    "print(f\"검증 데이터셋 크기: {val_df.count()} 행\")\n",
    "print(f\"테스트 데이터셋 크기: {test_df.count()} 행\")\n",
    "\n",
    "if train_df.count() > 0:\n",
    "    train_min_date, train_max_date = train_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  학습 데이터 기간: {train_min_date} ~ {train_max_date}\")\n",
    "if val_df.count() > 0:\n",
    "    val_min_date, val_max_date = val_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  검증 데이터 기간: {val_min_date} ~ {val_max_date}\")\n",
    "if test_df.count() > 0:\n",
    "    test_min_date, test_max_date = test_df.agg(spark_min(col(\"timestamp\")), spark_max(col(\"timestamp\"))).head()\n",
    "    print(f\"  테스트 데이터 기간: {test_min_date} ~ {test_max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c36c1a6-a5d9-4312-a11a-76ed67b0984a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# XGBoost Regressor 모델 정의\n",
    "xgb_regressor = SparkXGBRegressor(\n",
    "    features_col=\"features\", # 이미 assembled_df에 features 컬럼이 있으므로 이 이름을 사용\n",
    "    label_col=\"UHII\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 모델 학습 시에는 StringIndexer, OneHotEncoder, VectorAssembler는 이미 적용되었으므로\n",
    "# 모델 자체만 파이프라인에 넣거나, 바로 모델을 fit합니다.\n",
    "# 여기서는 Pipeline 대신 바로 모델을 fit하는 방식을 선택합니다.\n",
    "\n",
    "print(\"\\n--- XGBoost 모델 학습 시작 (assembled_df로부터) ---\")\n",
    "# assembled_df를 train_df, val_df, test_df로 분할했으므로,\n",
    "# 이제 train_df는 이미 features 컬럼을 가지고 있습니다.\n",
    "# 따라서, xgb_regressor 모델 인스턴스만 사용하여 학습을 수행합니다.\n",
    "model_xgb_fitted = xgb_regressor.fit(train_df) # train_df는 이미 features 컬럼을 가짐\n",
    "print(\"--- XGBoost 모델 학습 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b0cc94-50d8-40dd-971a-6e1021de6026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML이 찾아준 최적의 LightGBM 하이퍼파라미터\n",
    "space = {\n",
    "  \"colsample_bytree\": 0.7562949914479031,\n",
    "  \"learning_rate\": 0.340128033935979,\n",
    "  \"max_depth\": 7,\n",
    "  \"min_child_weight\": 15,\n",
    "  \"n_estimators\": 377,\n",
    "  \"n_jobs\": 100,\n",
    "  \"subsample\": 0.36705694207590633,\n",
    "  \"verbosity\": 0,\n",
    "  \"random_state\": 690069765,\n",
    "}\n",
    "\n",
    "# LightGBMRegressor 모델 정의\n",
    "# 파라미터 이름은 \"Best Trial Notebook\"에서 정확히 확인하는 것이 좋습니다.\n",
    "# 여기서는 synapse.ml.lightgbm의 일반적인 파라미터 이름을 사용합니다.\n",
    "lgbm_regressor = LGBMRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"UHII\",\n",
    "    numIterations=space[\"n_estimators\"], # n_estimators -> numIterations\n",
    "    learningRate=space[\"learning_rate\"],\n",
    "    maxDepth=space[\"max_depth\"],\n",
    "    numLeaves=space[\"num_leaves\"],\n",
    "    minChildSamples=space[\"min_child_samples\"],\n",
    "    subsample=space[\"subsample\"],\n",
    "    colsampleByTree=space[\"colsample_bytree\"], # colsampleByTree\n",
    "    regAlpha=space[\"lambda_l1\"], # lambda_l1 -> regAlpha\n",
    "    regLambda=space[\"lambda_l2\"], # lambda_l2 -> regLambda\n",
    "    seed=space[\"random_state\"],\n",
    "    # objective=\"regression\" # 명시적으로 회귀 문제임을 지정할 수 있습니다.\n",
    ")\n",
    "\n",
    "# LightGBM Estimator를 단일 스테이지 Pipeline에 포함\n",
    "# train_df는 이미 features와 UHII 컬럼을 가지고 있으므로, 추가적인 전처리 스테이지는 필요 없습니다.\n",
    "pipeline_lgbm = Pipeline(stages=[lgbm_regressor])\n",
    "\n",
    "print(\"\\n--- LightGBM 모델 학습 시작 (Pipeline 사용) ---\")\n",
    "# 파이프라인을 train_df에 fit합니다.\n",
    "# 파이프라인이 내부적으로 lgbm_regressor.fit(train_df)를 올바른 MLlib 방식으로 호출합니다.\n",
    "model_lgbm_fitted = pipeline_lgbm.fit(train_df)\n",
    "print(\"--- LightGBM 모델 학습 완료 ---\")\n",
    "\n",
    "# --- 모델 평가 (이전 코드와 동일하게 유지) ---\n",
    "# 검증 데이터셋으로 예측 수행\n",
    "predictions_val_lgbm = model_lgbm_fitted.transform(val_df)\n",
    "\n",
    "# 테스트 데이터셋으로 예측 수행\n",
    "predictions_test_lgbm = model_lgbm_fitted.transform(test_df)\n",
    "\n",
    "# RegressionEvaluator를 사용하여 모델 평가\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"UHII\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"\\n검증 데이터셋 RMSE (LightGBM): {rmse_val_lgbm:.4f}\")\n",
    "\n",
    "rmse_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 RMSE (LightGBM): {rmse_test_lgbm:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"r2\")\n",
    "r2_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"검증 데이터셋 R2 (LightGBM): {r2_val_lgbm:.4f}\")\n",
    "\n",
    "r2_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 R2 (LightGBM): {r2_test_lgbm:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae_val_lgbm = evaluator.evaluate(predictions_val_lgbm)\n",
    "print(f\"검증 데이터셋 MAE (LightGBM): {mae_val_lgbm:.4f}\")\n",
    "\n",
    "mae_test_lgbm = evaluator.evaluate(predictions_test_lgbm)\n",
    "print(f\"테스트 데이터셋 MAE (LightGBM): {mae_test_lgbm:.4f}\")\n",
    "\n",
    "print(\"\\n--- LightGBM 모델 평가 완료 ---\")\n",
    "predictions_test_lgbm.select(\"UHII\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec51f570-868f-4774-8e50-ed66deeec028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 검증 데이터셋으로 예측 수행\n",
    "# val_df 또한 이미 features 컬럼을 가지고 있습니다.\n",
    "predictions_val_xgb = model_lgbm_fitted.transform(val_df)\n",
    "\n",
    "# 테스트 데이터셋으로 예측 수행\n",
    "# test_df 또한 이미 features 컬럼을 가지고 있습니다.\n",
    "predictions_test_xgb = model_lgbm_fitted.transform(test_df)\n",
    "\n",
    "# RegressionEvaluator를 사용하여 모델 평가 (이 부분은 동일)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"UHII\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"\\n검증 데이터셋 RMSE (XGBoost): {rmse_val_xgb:.4f}\")\n",
    "\n",
    "rmse_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 RMSE (XGBoost): {rmse_test_xgb:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"r2\")\n",
    "r2_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"검증 데이터셋 R2 (XGBoost): {r2_val_xgb:.4f}\")\n",
    "\n",
    "r2_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 R2 (XGBoost): {r2_test_xgb:.4f}\")\n",
    "\n",
    "evaluator.setMetricName(\"mae\")\n",
    "mae_val_xgb = evaluator.evaluate(predictions_val_xgb)\n",
    "print(f\"검증 데이터셋 MAE (XGBoost): {mae_val_xgb:.4f}\")\n",
    "\n",
    "mae_test_xgb = evaluator.evaluate(predictions_test_xgb)\n",
    "print(f\"테스트 데이터셋 MAE (XGBoost): {mae_test_xgb:.4f}\")\n",
    "\n",
    "print(\"\\n--- XGBoost 모델 평가 완료 ---\")\n",
    "predictions_test_xgb.select(\"UHII\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddd5a94-db18-4721-b5e4-3ad77877cbf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- UHII와 수치형 피처 간의 상관관계 ---\")\n",
    "\n",
    "# UHII와 각 수치형 피처 간의 피어슨 상관계수 계산\n",
    "for col_name in numerical_cols:\n",
    "    try:\n",
    "        correlation = assembled_df.stat.corr(LABEL_COL, col_name)\n",
    "        print(f\"UHII 와 {col_name:<25}: {correlation:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating correlation for {col_name}: {e}\")\n",
    "\n",
    "# --- 수치형 피처 및 UHII 간의 상관관계 행렬 (Pandas 변환, 메모리 주의!) ---\n",
    "# 데이터셋의 크기가 크지 않다면 이 방법이 편리합니다.\n",
    "# 대용량 데이터셋에서는 Spark의 RDD 또는 VectorAssembler 후 행렬 연산 등의 고급 기법이 필요할 수 있습니다.\n",
    "# 여기서는 예시로 train_df를 사용합니다. 실제로는 전체 assembled_df를 사용하거나 적절히 샘플링하여 사용하세요.\n",
    "\n",
    "cols_for_corr_matrix = numerical_cols + [LABEL_COL]\n",
    "\n",
    "try:\n",
    "    # 필요한 컬럼만 선택하여 Pandas DataFrame으로 변환\n",
    "    pandas_df_for_corr = assembled_df.select(cols_for_corr_matrix).toPandas()\n",
    "\n",
    "    print(\"\\n--- 수치형 피처 및 UHII 간의 상관관계 행렬 (Pandas) ---\")\n",
    "    correlation_matrix = pandas_df_for_corr.corr(method='pearson')\n",
    "    print(correlation_matrix.round(2)) # 소수점 두 자리까지 표시\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError converting to Pandas for correlation matrix or calculation: {e}\")\n",
    "    print(\"Consider sampling the DataFrame or using Spark-native correlation methods if the dataset is too large.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
