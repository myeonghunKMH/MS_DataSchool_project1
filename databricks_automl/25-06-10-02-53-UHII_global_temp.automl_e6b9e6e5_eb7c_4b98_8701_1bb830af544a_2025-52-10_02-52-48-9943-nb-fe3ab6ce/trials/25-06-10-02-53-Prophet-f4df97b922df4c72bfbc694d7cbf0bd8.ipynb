{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ebeb3f-e9d5-42e1-a575-31450b2a549b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prophet training\n",
    "- This is an auto-generated notebook.\n",
    "- To reproduce these results, attach this notebook to a cluster with runtime version **15.4.x-cpu-ml-scala2.12**, and rerun it.\n",
    "- Compare trials in the [MLflow experiment](#mlflow/experiments/4326011134824737).\n",
    "- Clone this notebook into your project folder by selecting **File > Clone** in the notebook toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a57a05-61f5-431c-9805-5689f9d53c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import databricks.automl_runtime\n",
    "\n",
    "target_col = \"UHII\"\n",
    "time_col = \"timestamp\"\n",
    "unit = \"day\"\n",
    "\n",
    "id_cols = [\"District\"]\n",
    "\n",
    "horizon = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1af26c7d-75f4-403c-b2f3-ea5fa300f27a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0baf0c-3023-4110-8d5b-2d6d34177446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Create temp directory to download input data from MLflow\n",
    "input_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\n",
    "os.makedirs(input_temp_dir)\n",
    "\n",
    "# Download the artifact and read it into a pandas DataFrame\n",
    "input_data_path = mlflow.artifacts.download_artifacts(run_id=\"e4d9b882124846dc9b947f73116c189a\", artifact_path=\"data\", dst_path=input_temp_dir)\n",
    "\n",
    "input_file_path = os.path.join(input_data_path, \"training_data\")\n",
    "input_file_path = \"file://\" + input_file_path\n",
    "df_loaded = ps.from_pandas(pd.read_parquet(input_file_path))\n",
    "\n",
    "# Preview data\n",
    "display(df_loaded.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c3512a3-455f-47aa-970d-548ef714a9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aggregate data by `id_col` and `time_col`\n",
    "Group the data by `id_col` and `time_col`, and take average if there are multiple `target_col` values in the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "855255ca-aa4c-496f-b354-dea2688c0075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "group_cols = [time_col] + id_cols\n",
    "\n",
    "df_aggregated = df_loaded \\\n",
    "  .groupby(group_cols) \\\n",
    "  .agg(y=(target_col, \"avg\")) \\\n",
    "  .reset_index()\n",
    "\n",
    "df_aggregated = df_aggregated.assign(ts_id=lambda x:x[\"District\"].astype(str))\n",
    "\n",
    "\n",
    "display(df_aggregated.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d84e326-0f8c-48e8-a75a-af698754b138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Prophet model\n",
    "- Log relevant metrics to MLflow to track runs\n",
    "- All the runs are logged under [this MLflow experiment](#mlflow/experiments/4326011134824737)\n",
    "- Change the model parameters and re-run the training cell to log a different trial to the MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8b5ba8-b79e-46f0-b36d-09087b5336b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b664a5-e01b-4a75-8055-7e25d15a3c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# disable informational messages from prophet\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d320d055-250a-4418-aac5-42b60cbb420e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_schema = df_aggregated.to_spark().schema\n",
    "result_columns = id_cols + [\"model_json\", \"prophet_params\", \"start_time\", \"end_time\", \"mse\",\n",
    "                  \"rmse\", \"mae\", \"mape\", \"mdape\", \"smape\", \"coverage\"]\n",
    "result_schema = StructType(\n",
    "  [StructField(id_col, df_schema[id_col].dataType) for id_col in id_cols] + [\n",
    "  StructField(\"model_json\", StringType()),\n",
    "  StructField(\"prophet_params\", StringType()),\n",
    "  StructField(\"start_time\", TimestampType()),\n",
    "  StructField(\"end_time\", TimestampType()),\n",
    "  StructField(\"mse\", FloatType()),\n",
    "  StructField(\"rmse\", FloatType()),\n",
    "  StructField(\"mae\", FloatType()),\n",
    "  StructField(\"mape\", FloatType()),\n",
    "  StructField(\"mdape\", FloatType()),\n",
    "  StructField(\"smape\", FloatType()),\n",
    "  StructField(\"coverage\", FloatType())\n",
    "  ])\n",
    "\n",
    "def prophet_training(history_pd):\n",
    "  from hyperopt import hp\n",
    "  from databricks.automl_runtime.forecast.prophet.forecast import ProphetHyperoptEstimator\n",
    "\n",
    "  seasonality_mode = [\"additive\", \"multiplicative\"]\n",
    "  search_space =  {\n",
    "    \"changepoint_prior_scale\": hp.loguniform(\"changepoint_prior_scale\", -6.9, -0.69),\n",
    "    \"seasonality_prior_scale\": hp.loguniform(\"seasonality_prior_scale\", -6.9, 2.3),\n",
    "    \"holidays_prior_scale\": hp.loguniform(\"holidays_prior_scale\", -6.9, 2.3),\n",
    "    \"seasonality_mode\": hp.choice(\"seasonality_mode\", seasonality_mode)\n",
    "  }\n",
    "  country_holidays = None\n",
    "  run_parallel = False\n",
    " \n",
    "  hyperopt_estim = ProphetHyperoptEstimator(horizon=horizon, frequency_unit=unit, metric=\"rmse\",interval_width=0.8,\n",
    "                   country_holidays=country_holidays, search_space=search_space, num_folds=5, max_eval=10, trial_timeout=1692,\n",
    "                   random_state=583222064, is_parallel=run_parallel)\n",
    "\n",
    "  results_pd = hyperopt_estim.fit(history_pd)\n",
    "  results_pd[id_cols] = history_pd[id_cols]\n",
    "  results_pd[\"start_time\"] = pd.Timestamp(history_pd[\"ds\"].min())\n",
    "  results_pd[\"end_time\"] = pd.Timestamp(history_pd[\"ds\"].max())\n",
    " \n",
    "  return results_pd[result_columns]\n",
    "\n",
    "def train_with_fail_safe(df):\n",
    "  try:\n",
    "    return prophet_training(df)\n",
    "  except Exception as e:\n",
    "    print(f\"Encountered an exception while training timeseries: {repr(e)}\")\n",
    "    return pd.DataFrame(columns=result_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fee6cb-485f-406b-8e7f-b4ef36076567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from databricks.automl_runtime.forecast.prophet.model import mlflow_prophet_log_model, MultiSeriesProphetModel\n",
    "\n",
    "with mlflow.start_run(experiment_id=\"4326011134824737\", run_name=\"Prophet\") as mlflow_run:\n",
    "  mlflow.set_tag(\"estimator_name\", \"Prophet\")\n",
    "  mlflow.log_param(\"interval_width\", 0.8)\n",
    "  df_aggregated = df_aggregated.rename(columns={time_col: \"ds\"})\n",
    "\n",
    "  forecast_results = (df_aggregated.to_spark().repartition(sc.defaultParallelism, \"ts_id\") \\\n",
    "    .groupby(\"ts_id\").applyInPandas(train_with_fail_safe, result_schema)).cache().pandas_api()\n",
    "  results_pdf = forecast_results[id_cols + [\"model_json\", \"start_time\", \"end_time\"]].to_pandas()\n",
    "  results_pdf[\"ts_id\"] = results_pdf[id_cols].astype(str).agg('-'.join, axis=1)\n",
    "  results_pdf[\"ts_id_tuple\"] = results_pdf[id_cols].apply(tuple, axis=1)\n",
    "   \n",
    "  # Check whether every time series's model is trained\n",
    "  ts_models_trained = set(results_pdf[\"ts_id\"].unique().tolist())\n",
    "  ts_ids = set(df_aggregated[\"ts_id\"].unique().tolist())\n",
    "\n",
    "  if len(ts_models_trained) == 0:\n",
    "    raise Exception(\"Trial unable to train models for any identities. Please check the training cell for error details\")\n",
    "\n",
    "  if ts_ids != ts_models_trained:\n",
    "    mlflow.log_param(\"partial_model\", True)\n",
    "    print(f\"WARNING: Models not trained for the following identities: {ts_ids.difference(ts_models_trained)}\")\n",
    " \n",
    "  # Log the metrics to mlflow\n",
    "  metric_name_map = {\"mse\": \"mean_squared_error\", \"rmse\": \"root_mean_squared_error\", \"mae\": \"mean_absolute_error\",\n",
    "                     \"mape\": \"mean_absolute_percentage_error\", \"mdape\": \"mdape\", \"smape\": \"smape\", \"coverage\": \"coverage\"}\n",
    "  avg_metrics = forecast_results[metric_name_map.keys()].rename(columns=metric_name_map).mean().to_frame(name=\"mean_metrics\").reset_index()\n",
    "  avg_metrics[\"index\"] = \"val_\" + avg_metrics[\"index\"].astype(str)\n",
    "  avg_metrics.set_index(\"index\", inplace=True)\n",
    "  mlflow.log_metrics(avg_metrics.to_dict()[\"mean_metrics\"])\n",
    "\n",
    "  # Create mlflow prophet model\n",
    "  results_pdf = results_pdf.set_index(\"ts_id_tuple\")\n",
    "  model_json = results_pdf[\"model_json\"].to_dict()\n",
    "  start_time = results_pdf[\"start_time\"].to_dict()\n",
    "  end_time = results_pdf[\"end_time\"].to_dict()\n",
    "  end_history_time = max(end_time.values())\n",
    "  prophet_model = MultiSeriesProphetModel(model_json, start_time, end_history_time, horizon, unit, time_col, id_cols)\n",
    "\n",
    "  # Generate sample input dataframe\n",
    "  sample_input = df_loaded.head(1).to_pandas()\n",
    "  sample_input[time_col] = pd.to_datetime(sample_input[time_col])\n",
    "  sample_input.drop(columns=[target_col], inplace=True)\n",
    "\n",
    "  mlflow_prophet_log_model(prophet_model, sample_input=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809dc9fb-043f-4bf7-8860-67b06b447d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show stats of forecast_results. By default we do not display it because the output takes up a lot of storage for large datasets.\n",
    "# Change this line to `forecast_results.head()` and re-run the notebook to check the result.\n",
    "forecast_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86381a51-69ad-4108-9f38-e5023402b55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a105d55-8400-4372-a419-0927267893fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "run_id = mlflow_run.info.run_id\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f31dfe2-18a3-4b53-b2b5-0bd13d1547ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = loaded_model._model_impl.python_model\n",
    "col_types = [StructField(f\"{n}\", FloatType()) for n in model.get_reserved_cols()]\n",
    "col_types.append(StructField(\"ds\",TimestampType()))\n",
    "col_types.append(StructField(\"ts_id\",StringType()))\n",
    "result_schema = StructType(col_types)\n",
    "\n",
    "future_df = model.make_future_dataframe(include_history=False)\n",
    "future_df[\"ts_id\"] = future_df[id_cols].apply(tuple, axis=1)\n",
    "future_df = future_df.rename(columns={time_col: \"ds\"})\n",
    "display(future_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a37d8dc3-00a9-40ce-a574-443c72efe4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict future with the default horizon\n",
    "forecast_pd = future_df.groupby(id_cols).apply(lambda df: model._predict_impl(df, model._horizon)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3a74130-a535-4a73-8ba4-d3034a7e867e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plotly plots is turned off by default because it takes up a lot of storage.\n",
    "# Set this flag to True and re-run the notebook to see the interactive plots with plotly\n",
    "use_plotly = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bfdc432-3cbc-49b9-8bcb-088e3ff9765a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose a random id for plot\n",
    "forecast_pd[\"ts_id\"] = forecast_pd[id_cols].apply(tuple, axis=1)\n",
    "id = set(forecast_pd.index.to_list()).pop()\n",
    "ts_id = forecast_pd[\"ts_id\"].loc[id]\n",
    "# Get the prophet model for plot\n",
    "model = loaded_model._model_impl.python_model.model(ts_id)\n",
    "predict_pd = forecast_pd[forecast_pd[\"ts_id\"] == ts_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7153b9f0-917d-431d-a20a-c89f407b8529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Plot the forecast with change points and trend\n",
    "Plot the forecast using the `plot` method with your forecast dataframe. You can use `prophet.plot.add_changepoints_to_plot` to overlay significant changepoints. An interactive figure can be created with plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6866e9b3-78d1-4ebb-a9d2-00732b6cfa36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet.plot import add_changepoints_to_plot, plot_plotly\n",
    "\n",
    "if use_plotly:\n",
    "    fig = plot_plotly(model, predict_pd, changepoints=True, trend=True, figsize=(1200, 600))\n",
    "else:\n",
    "    fig = model.plot(predict_pd)\n",
    "    a = add_changepoints_to_plot(fig.gca(), model, predict_pd)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2de06c45-5ac1-4169-8725-d5ef79e34ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Plot the forecast components\n",
    "Use the `Prophet.plot_components` method to see the components. By default you'll see the trend, yearly seasonality, and weekly seasonality of the time series. You can also include holidays. An interactive figure can be created with plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5741f99-0b27-47cd-b1e7-5783db18d40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from prophet.plot import plot_components_plotly\n",
    "if use_plotly:\n",
    "    fig = plot_components_plotly(model, predict_pd, figsize=(900, 400))\n",
    "    fig.show()\n",
    "else:\n",
    "    fig = model.plot_components(predict_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9149360-1d78-4275-a9d9-764d9c31c120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11c1bbc7-fc5d-4338-93bd-c144429bc2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_cols = id_cols + [\"ds\", \"yhat\"]\n",
    "forecast_pd = forecast_pd.reset_index()\n",
    "display(forecast_pd[predict_cols].tail(horizon))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "25-06-10-02:53-Prophet-f4df97b922df4c72bfbc694d7cbf0bd8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "name": "Prophet-f4df97b922df4c72bfbc694d7cbf0bd8"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
